# Team Rocket

## How it works:
A Hierarchical Model for Data-to-Text Generation is used at the core. Keeping in mind, that the proposed solution should be able to adapt to new datasets from different domains, we choose sequence-to-sequence model, with self attention mechanism.

For it, we needed to follow encoder-decoder based approach, where tabular data is given to decoder, and a textual content is generated by the encoder.

### Tasks
```1. Create a dataset, with tabular data (CSV) as an input, and stories as the output.

For this, we scrapped news articles from the internet, and manually replaced data points with place holders.
These placeholders are nothing but the columns provided in input dataset.
Sentences emphasising similar meaning were grouped together, and a textual vocab. was created.
Further, the placeholders were replace with actual data points, and random stories were generated.
Around 1700 articles were created to train the model.
```

```
2. Choose the techstack & framework. 

We used PyTorch and  OpenNMT framework for it. The model had already shown SOTA results for various datasets, and we expected similar results for our task.

```


Note:
The model was trained for  around 1000 epochs, due to hardware and time constraints, and 68 perc accuracy was achieved.


```
3. Training & Deployment

After training for around 1000 epochs, we paused the training part. We planned to resume it once we were done with the UI, and model deployment.
To make it compatible on different platforms for testing purposes, we divided our project into a bunch of microservices.

a. Translator ( Which takes in vectorised input, and generated articles. Basically, NLG's core module
b. Fronend: Angular app with nginx to serve statc files
c. RestAPI: A REST APIService, which would handle file uploads, and preprocess the CSV to required vector format. This acts a mediator between the frontend Translator. Python's FastAPI & Flask were used.


All these microservices were dockerised, and docker-compose was used to run all them together as a cluster.

PS: our app had two modules (Stocks & Pollution). And each one is hosted on a different servers. Nginx is used to showcase them together as a single app.
```



## Techstack used:
1. PyTorch
2. Openmnt
3. FastAPI
4. Flask
5. nginx
6. Angular
7. Pandas, scipy, numpy, and other helper libraries
8. Docker

## Refrence:
[A Hierarchical Model for Data-to-Text Generation](https://arxiv.org/abs/1912.10011) (Rebuffel, Soulier, Scoutheeten, Gallinari; ECIR 2020) paper was used as a refrencem. Most of this code is based on [OpenNMT](https://github.com/OpenNMT/OpenNMT-py).


# How to use it

## Requirements

1. docker
2. docker-compose

## Instructions:
Note: These instructions are specifically meant for stock dataset. 

## To run locally, as a webapp
1. Clone the repo.
2. Make sure you have sudo rights, and port 80 , 5000 & 8001 are not used by any other process
3. Run docker-compose up inside the cloned directory.
4. Go to http://localhost:80 to access the app.
Note: We have hosted it on: http://35.222.191.38/ as well.
5. In case the remote URL is down, please let us know. 

## To run it on your terminal
1. Clone the repo.
2. Run docker-compose run engine (cmd)
3. This would allow you to (train/test/preprocess) your input
4. 'app/model/model.pt' is the last saved checkpoint.
5. 'app/translate.cfg' contains parameters that can be changed (article's output length, random seed, batch size, etc).
`docker-compose run engine 'python translate.py --config translate.cfg'
This would take input from 'app/model/test.txt', and generate output in 'app/model/test_results.txt'
Note: inpput has to be in predefined format, and not in CSV. To generate results from CSV, data would require preprocessing.


## URL to hosted app:
1. http://teamrocket-hackhastle-1879085481.ap-south-1.elb.amazonaws.com/ (Pollution)
2. http://35.222.191.38/ (Stock)


